# RayJob: Download SpamAssassin Dataset
# 
# This job downloads the SpamAssassin public corpus and uploads to Azure Blob Storage.
# 
# Prerequisites:
#   - ray-azure-secret created in ray namespace
#   - Ray cluster running (ray-cluster)
#
# Usage:
#   kubectl apply -f rayjob-download-dataset.yaml
#
# Monitor:
#   kubectl logs -n ray -l job-name=download-dataset -f

apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: download-dataset
  namespace: ray
  labels:
    app: spam-detection
    phase: data-ingestion
spec:
  entrypoint: python /home/ray/download_dataset.py
  
  # Use existing Ray cluster
  clusterSelector:
    ray.io/cluster: ray-cluster
  
  # Job configuration
  runtimeEnvYAML: |
    pip:
      - adlfs>=2023.1.0
      - azure-identity>=1.12.0
    working_dir: "."
    env_vars:
      AZURE_STORAGE_ACCOUNT_NAME: "${AZURE_STORAGE_ACCOUNT_NAME}"
      AZURE_STORAGE_ACCOUNT_KEY: "${AZURE_STORAGE_ACCOUNT_KEY}"
  
  # Timeout settings
  ttlSecondsAfterFinished: 86400  # Clean up after 24 hours
  # activeDeadlineSeconds: 3600     # 1 hour timeout
  
  # Submitter pod configuration
  submitterPodTemplate:
    spec:
      containers:
        - name: ray-job-submitter
          image: rayproject/ray-ml:2.9.0-py310
          env:
            - name: AZURE_STORAGE_ACCOUNT_NAME
              valueFrom:
                secretKeyRef:
                  name: ray-azure-secret
                  key: azure-storage-account-name
            - name: AZURE_STORAGE_ACCOUNT_KEY
              valueFrom:
                secretKeyRef:
                  name: ray-azure-secret
                  key: azure-storage-account-key
          volumeMounts:
            - name: job-scripts
              mountPath: /home/ray
      volumes:
        - name: job-scripts
          configMap:
            name: download-dataset-scripts
      restartPolicy: Never

---
# ConfigMap containing the Python script
apiVersion: v1
kind: ConfigMap
metadata:
  name: download-dataset-scripts
  namespace: ray
data:
  download_dataset.py: |
    """
    Download SpamAssassin Public Corpus and upload to Azure Blob Storage.
    """
    
    import os
    import tarfile
    import tempfile
    import logging
    from pathlib import Path
    from urllib.request import urlretrieve
    
    import ray
    from adlfs import AzureBlobFileSystem
    
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    CORPUS_URLS = {
        "spam": "https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2",
        "spam_2": "https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2",
        "ham": "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2",
        "ham_2": "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2",
    }
    
    STORAGE_ACCOUNT_NAME = os.environ.get("AZURE_STORAGE_ACCOUNT_NAME")
    STORAGE_ACCOUNT_KEY = os.environ.get("AZURE_STORAGE_ACCOUNT_KEY")
    CONTAINER_NAME = "datasets"
    TARGET_PATH = "raw/spamassassin"
    
    
    def get_azure_fs() -> AzureBlobFileSystem:
        return AzureBlobFileSystem(
            account_name=STORAGE_ACCOUNT_NAME,
            account_key=STORAGE_ACCOUNT_KEY
        )
    
    
    def download_and_extract(url: str, extract_dir: Path) -> Path:
        logger.info(f"Downloading {url}")
        archive_path = extract_dir / "archive.tar.bz2"
        urlretrieve(url, archive_path)
        
        logger.info(f"Extracting to {extract_dir}")
        with tarfile.open(archive_path, "r:bz2") as tar:
            tar.extractall(extract_dir)
        
        archive_path.unlink()
        extracted_dirs = [d for d in extract_dir.iterdir() if d.is_dir()]
        return extracted_dirs[0] if extracted_dirs else extract_dir
    
    
    @ray.remote
    def process_corpus(corpus_name: str, url: str) -> dict:
        fs = get_azure_fs()
        stats = {"corpus": corpus_name, "files_uploaded": 0, "errors": 0}
        
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)
            
            try:
                extracted_dir = download_and_extract(url, tmpdir_path)
            except Exception as e:
                logger.error(f"Failed to download/extract {corpus_name}: {e}")
                stats["errors"] += 1
                return stats
            
            label = "spam" if "spam" in corpus_name else "ham"
            
            for email_file in extracted_dir.rglob("*"):
                if email_file.is_file() and not email_file.name.startswith("."):
                    try:
                        content = email_file.read_bytes()
                        target_blob = f"{CONTAINER_NAME}/{TARGET_PATH}/{label}/{corpus_name}_{email_file.name}"
                        
                        with fs.open(target_blob, "wb") as f:
                            f.write(content)
                        
                        stats["files_uploaded"] += 1
                    except Exception as e:
                        logger.warning(f"Failed to upload {email_file.name}: {e}")
                        stats["errors"] += 1
            
            logger.info(f"Completed {corpus_name}: {stats['files_uploaded']} files uploaded")
        
        return stats
    
    
    def main():
        logger.info("Starting SpamAssassin corpus download")
        
        if not STORAGE_ACCOUNT_NAME or not STORAGE_ACCOUNT_KEY:
            raise ValueError("Azure storage credentials not set")
        
        ray.init(address="auto")
        logger.info(f"Connected to Ray cluster: {ray.cluster_resources()}")
        
        futures = [
            process_corpus.remote(name, url) 
            for name, url in CORPUS_URLS.items()
        ]
        
        results = ray.get(futures)
        
        total_files = sum(r["files_uploaded"] for r in results)
        total_errors = sum(r["errors"] for r in results)
        
        logger.info(f"Download complete: {total_files} files uploaded, {total_errors} errors")
        for result in results:
            logger.info(f"  {result['corpus']}: {result['files_uploaded']} files")
    
    
    if __name__ == "__main__":
        main()
