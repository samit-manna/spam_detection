# Model Deployment Workflow
# Deploys ML models from MLflow to KServe InferenceService
#
# This workflow:
# 1. Verifies model exists in MLflow Staging stage
# 2. Exports model to ONNX format and uploads to blob storage
# 3. Deploys to KServe staging InferenceService
# 4. Runs smoke tests
# 5. (Optional) Promotes to production after approval

name: Model Deploy

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model name in MLflow'
        required: true
        default: 'spam-detector'
      deploy_to_production:
        description: 'Also deploy to production after staging tests pass'
        required: false
        default: false
        type: boolean
      image_tag:
        description: 'Docker image tag for model-export'
        required: false
        default: 'v0.35'

env:
  KSERVE_NAMESPACE: kserve
  MLFLOW_NAMESPACE: mlflow
  IMAGE_TAG: ${{ github.event.inputs.image_tag || 'v0.35' }}

jobs:
  # ===========================================================================
  # Step 1: Verify Model in Staging
  # ===========================================================================
  verify-staging-model:
    name: Verify Staging Model
    runs-on: ml-platform-runners
    
    outputs:
      model_version: ${{ steps.model-info.outputs.version }}
      model_run_id: ${{ steps.model-info.outputs.run_id }}
      acr_name: ${{ steps.get-acr.outputs.acr_name }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Get ACR Name
        id: get-acr
        run: |
          # Try to get ACR name from platform-config ConfigMap
          ACR_NAME=$(kubectl get configmap platform-config -n ${{ env.KSERVE_NAMESPACE }} \
            -o jsonpath='{.data.ACR_NAME}' 2>/dev/null || echo "")
          
          # Fallback: try github-runners namespace
          if [ -z "$ACR_NAME" ]; then
            ACR_NAME=$(kubectl get configmap platform-config -n github-runners \
              -o jsonpath='{.data.ACR_NAME}' 2>/dev/null || echo "")
          fi
          
          # Fallback: try to get from existing deployments in serving namespace
          if [ -z "$ACR_NAME" ]; then
            ACR_NAME=$(kubectl get deployment -n serving \
              -o jsonpath='{.items[0].spec.template.spec.containers[0].image}' 2>/dev/null | cut -d'/' -f1 || echo "")
          fi
          
          if [ -z "$ACR_NAME" ]; then
            echo "âŒ Could not determine ACR name"
            echo "   Please ensure platform-config ConfigMap exists or deployments are present"
            exit 1
          fi
          
          echo "acr_name=${ACR_NAME}" >> $GITHUB_OUTPUT
          echo "âœ… ACR: ${ACR_NAME}"

      - name: Get Model in Staging Stage
        id: model-info
        env:
          MLFLOW_TRACKING_URI: http://mlflow-service.${{ env.MLFLOW_NAMESPACE }}.svc.cluster.local:5000
        run: |
          echo "ðŸ” Looking for model in Staging stage..."
          
          # Use model_lifecycle.py to get model info
          OUTPUT=$(python model-serving/scripts/model_lifecycle.py get-model-info \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Staging \
            --format json \
            --mlflow-uri "$MLFLOW_TRACKING_URI")
          
          echo "Response: $OUTPUT"
          
          # Parse JSON output
          FOUND=$(echo "$OUTPUT" | python -c "import sys, json; d=json.load(sys.stdin); print(d.get('found', False))")
          
          if [ "$FOUND" != "True" ]; then
            echo "âŒ No model found in Staging stage!"
            echo "   Please ensure the Kubeflow training pipeline has completed"
            echo "   and promoted a model to Staging."
            exit 1
          fi
          
          VERSION=$(echo "$OUTPUT" | python -c "import sys, json; d=json.load(sys.stdin); print(d.get('version', ''))")
          RUN_ID=$(echo "$OUTPUT" | python -c "import sys, json; d=json.load(sys.stdin); print(d.get('run_id', ''))")
          
          echo "âœ… Found model version ${VERSION} in Staging"
          echo "   Run ID: ${RUN_ID}"
          
          echo "version=${VERSION}" >> $GITHUB_OUTPUT
          echo "run_id=${RUN_ID}" >> $GITHUB_OUTPUT

      - name: Summary
        run: |
          echo "## ðŸ“‹ Staging Model Verification" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Model Name | ${{ github.event.inputs.model_name }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Version | ${{ steps.model-info.outputs.version }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run ID | ${{ steps.model-info.outputs.run_id }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ACR | ${{ steps.get-acr.outputs.acr_name }} |" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Step 2: Export Model to ONNX
  # ===========================================================================
  export-model:
    name: Export Model
    runs-on: ml-platform-runners
    needs: verify-staging-model
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Export Model to ONNX
        env:
          ACR_NAME: ${{ needs.verify-staging-model.outputs.acr_name }}
        run: |
          echo "ðŸš€ Exporting model to ONNX format..."
          
          python model-serving/scripts/model_lifecycle.py export \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Staging \
            --acr-name "$ACR_NAME" \
            --image-tag "${{ env.IMAGE_TAG }}" \
            --namespace "${{ env.KSERVE_NAMESPACE }}" \
            --timeout 300

      - name: Summary
        run: |
          echo "## ðŸ“¦ Model Export" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Model exported to ONNX format and uploaded to blob storage" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Step 3: Deploy to Staging
  # ===========================================================================
  deploy-staging:
    name: Deploy to Staging
    runs-on: ml-platform-runners
    needs: [verify-staging-model, export-model]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Deploy to Staging KServe
        run: |
          echo "ðŸš€ Deploying to staging KServe..."
          
          python model-serving/scripts/model_lifecycle.py deploy \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Staging \
            --namespace "${{ env.KSERVE_NAMESPACE }}" \
            --manifest-dir "model-serving/inference-service" \
            --timeout 300

      - name: Summary
        run: |
          echo "## ðŸš€ Staging Deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Model deployed to staging InferenceService" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Name |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|------|" >> $GITHUB_STEP_SUMMARY
          echo "| InferenceService | spam-detector-staging |" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Step 4: Smoke Tests
  # ===========================================================================
  smoke-tests:
    name: Smoke Tests
    runs-on: ml-platform-runners
    needs: [verify-staging-model, deploy-staging]
    
    outputs:
      test_passed: ${{ steps.smoke-test.outputs.passed }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Run Smoke Tests
        id: smoke-test
        run: |
          echo "ðŸ§ª Running smoke tests against staging model..."
          
          python model-serving/scripts/model_lifecycle.py smoke-test \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Staging \
            --namespace "${{ env.KSERVE_NAMESPACE }}"
          
          echo "passed=true" >> $GITHUB_OUTPUT

      - name: Summary
        if: always()
        run: |
          echo "## ðŸ§ª Smoke Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.smoke-test.outputs.passed }}" == "true" ]; then
            echo "âœ… All smoke tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Smoke tests failed" >> $GITHUB_STEP_SUMMARY
          fi

  # ===========================================================================
  # Step 5: Deploy to Production (Optional)
  # ===========================================================================
  verify-production-model:
    name: Verify Production Model
    runs-on: ml-platform-runners
    needs: smoke-tests
    if: ${{ github.event.inputs.deploy_to_production == 'true' }}
    
    outputs:
      model_version: ${{ steps.model-info.outputs.version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Check for Model in Production Stage
        id: model-info
        env:
          MLFLOW_TRACKING_URI: http://mlflow-service.${{ env.MLFLOW_NAMESPACE }}.svc.cluster.local:5000
        run: |
          echo "ðŸ” Checking for model in Production stage..."
          
          OUTPUT=$(python model-serving/scripts/model_lifecycle.py get-model-info \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Production \
            --format json \
            --mlflow-uri "$MLFLOW_TRACKING_URI" 2>/dev/null || echo '{"found": false}')
          
          FOUND=$(echo "$OUTPUT" | python -c "import sys, json; d=json.load(sys.stdin); print(d.get('found', False))")
          
          if [ "$FOUND" != "True" ]; then
            echo "âš ï¸ No model currently in Production stage"
            echo "   This will be the first production deployment"
            echo "version=" >> $GITHUB_OUTPUT
          else
            VERSION=$(echo "$OUTPUT" | python -c "import sys, json; d=json.load(sys.stdin); print(d.get('version', ''))")
            echo "ðŸ“Œ Current production version: ${VERSION}"
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
          fi

  deploy-production:
    name: Deploy to Production
    runs-on: ml-platform-runners
    needs: [verify-staging-model, smoke-tests, verify-production-model]
    if: ${{ github.event.inputs.deploy_to_production == 'true' }}
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Promote Model to Production in MLflow
        env:
          MLFLOW_TRACKING_URI: http://mlflow-service.${{ env.MLFLOW_NAMESPACE }}.svc.cluster.local:5000
        run: |
          echo "ðŸ“¤ Promoting model v${{ needs.verify-staging-model.outputs.model_version }} to Production..."
          
          python model-serving/scripts/model_lifecycle.py promote \
            --model-name "${{ github.event.inputs.model_name }}" \
            --version "${{ needs.verify-staging-model.outputs.model_version }}" \
            --stage Production \
            --mlflow-uri "$MLFLOW_TRACKING_URI" \
            -y

      - name: Export Model for Production
        env:
          ACR_NAME: ${{ needs.verify-staging-model.outputs.acr_name }}
        run: |
          echo "ðŸ“¦ Exporting model for production..."
          
          python model-serving/scripts/model_lifecycle.py export \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Production \
            --acr-name "$ACR_NAME" \
            --image-tag "${{ env.IMAGE_TAG }}" \
            --namespace "${{ env.KSERVE_NAMESPACE }}" \
            --timeout 300

      - name: Deploy to Production KServe
        run: |
          echo "ðŸš€ Deploying to production KServe..."
          
          python model-serving/scripts/model_lifecycle.py deploy \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Production \
            --namespace "${{ env.KSERVE_NAMESPACE }}" \
            --manifest-dir "model-serving/inference-service" \
            --timeout 300

      - name: Verify Production Deployment
        run: |
          echo "âœ… Verifying production deployment..."
          
          python model-serving/scripts/model_lifecycle.py smoke-test \
            --model-name "${{ github.event.inputs.model_name }}" \
            --stage Production \
            --namespace "${{ env.KSERVE_NAMESPACE }}"

      - name: Summary
        run: |
          echo "## ðŸ­ Production Deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Model v${{ needs.verify-staging-model.outputs.model_version }} deployed to production" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Version | ${{ needs.verify-staging-model.outputs.model_version }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Previous Version | ${{ needs.verify-production-model.outputs.model_version || 'None' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| InferenceService | spam-detector |" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Final Summary
  # ===========================================================================
  summary:
    name: Deployment Summary
    runs-on: ml-platform-runners
    needs: [verify-staging-model, export-model, deploy-staging, smoke-tests]
    if: always()
    
    steps:
      - name: Generate Summary
        run: |
          echo "## ðŸ“Š Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Model Information" >> $GITHUB_STEP_SUMMARY
          echo "- **Model Name:** ${{ github.event.inputs.model_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Version:** ${{ needs.verify-staging-model.outputs.model_version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Image Tag:** ${{ env.IMAGE_TAG }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Verify Model | ${{ needs.verify-staging-model.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Export Model | ${{ needs.export-model.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy Staging | ${{ needs.deploy-staging.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ needs.smoke-tests.result }} |" >> $GITHUB_STEP_SUMMARY
