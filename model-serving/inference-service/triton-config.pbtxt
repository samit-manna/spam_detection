# Triton Inference Server Model Configuration
#
# This config.pbtxt defines the model configuration for Triton.
# It specifies input/output shapes, data types, and optimization settings.
#
# Template variables:
#   MODEL_NAME: Name of the model (spam-detector)
#   NUM_FEATURES: Number of input features (524)
#
# This file is auto-generated by model-export/export_model.py
# Manual edits may be overwritten.

name: "spam-detector"
platform: "onnxruntime_onnx"
# Use max_batch_size: 0 for explicit batch dimension handling
# This avoids shape mismatch errors with ONNX model outputs where:
#   - ONNX outputs 'label' as shape [-1] (1D tensor)
#   - With max_batch_size > 0, Triton expects [-1, 1] (2D with implicit batch)
max_batch_size: 0

# Input configuration
# The model expects a 2D tensor of shape [batch_size, 524]
# where 524 features are:
#   - 8 text features (url_count, uppercase_ratio, etc.)
#   - 8 structural features (has_html, subject_length, etc.)
#   - 4 temporal features (hour_of_day, day_of_week, etc.)
#   - 500 TF-IDF features
#   - 4 sender domain features (email_count, spam_count, ham_count, spam_ratio)
# Note: Using explicit batch dimension [-1, 524] since max_batch_size: 0
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [-1, 524]
  }
]

# Output configuration
# label: Binary class label (0=ham, 1=spam)
# probabilities: Probability distribution [P(ham), P(spam)]
# Note: Using explicit batch dimensions since max_batch_size: 0
output [
  {
    name: "label"
    data_type: TYPE_INT64
    dims: [-1]
  },
  {
    name: "probabilities"
    data_type: TYPE_FP32
    dims: [-1, 2]
  }
]

# Instance group configuration
# Specifies how many instances of the model to run and on what hardware
instance_group [
  {
    count: 1
    kind: KIND_CPU
    # For GPU inference, use:
    # kind: KIND_GPU
    # gpus: [0]
  }
]

# Dynamic batching configuration
# Enables automatic batching of incoming requests for better throughput
dynamic_batching {
  # Preferred batch sizes for optimal performance
  preferred_batch_size: [8, 16, 32, 64]
  
  # Maximum time to wait for forming a batch (microseconds)
  max_queue_delay_microseconds: 100
  
  # Priority levels (optional)
  # priority_levels: 2
  # default_priority_level: 1
  
  # Queue policy
  default_queue_policy {
    timeout_action: REJECT
    default_timeout_microseconds: 100000  # 100ms
    allow_timeout_override: true
    max_queue_size: 100
  }
}

# Model warmup configuration
# Runs warmup inference on model load to initialize optimizations
model_warmup [
  {
    name: "warmup_sample"
    batch_size: 1
    inputs {
      key: "input"
      value {
        data_type: TYPE_FP32
        dims: [524]
        random_data: true
      }
    }
  },
  {
    name: "warmup_batch"
    batch_size: 32
    inputs {
      key: "input"
      value {
        data_type: TYPE_FP32
        dims: [524]
        random_data: true
      }
    }
  }
]

# Version policy
# Controls which model versions are available for inference
version_policy {
  latest {
    num_versions: 2
  }
  # Alternative: specific versions
  # specific { versions: [1, 2] }
  # Alternative: all versions
  # all {}
}

# Optimization settings
optimization {
  # ONNX Runtime specific optimizations
  execution_accelerators {
    cpu_execution_accelerator: [
      {
        name: "openvino"
        # OpenVINO parameters (optional)
        # parameters { key: "device_type" value: "CPU" }
      }
    ]
    # For GPU inference:
    # gpu_execution_accelerator: [
    #   {
    #     name: "tensorrt"
    #     parameters { key: "precision_mode" value: "FP16" }
    #     parameters { key: "max_workspace_size_bytes" value: "1073741824" }
    #   }
    # ]
  }
  
  # Input pinning for better memory performance
  input_pinned_memory { enable: true }
  
  # Output pinning
  output_pinned_memory { enable: true }
  
  # Graph optimization level for ONNX Runtime
  # 0: disable all optimizations
  # 1: enable basic optimizations
  # 2: enable extended optimizations
  # 99: enable all optimizations
  graph { level: 99 }
}

# Response cache (for repeated identical requests)
response_cache {
  enable: true
}

# Parameters for custom processing (optional)
parameters {
  key: "model_type"
  value { string_value: "xgboost" }
}
parameters {
  key: "task"
  value { string_value: "binary_classification" }
}
parameters {
  key: "num_features"
  value { string_value: "524" }
}

# Model transaction policy
# Controls how model updates are handled
model_transaction_policy {
  decoupled: false
}
